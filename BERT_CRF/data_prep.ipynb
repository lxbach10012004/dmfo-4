{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd5903c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T14:11:40.491807Z",
     "iopub.status.busy": "2024-12-03T14:11:40.491436Z",
     "iopub.status.idle": "2024-12-03T14:18:51.360304Z",
     "shell.execute_reply": "2024-12-03T14:18:51.359343Z"
    },
    "papermill": {
     "duration": 430.874706,
     "end_time": "2024-12-03T14:18:51.362615",
     "exception": false,
     "start_time": "2024-12-03T14:11:40.487909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset saved as raw_dataset.csv\n",
      "Raw train and test datasets saved as raw_train.csv and raw_test.csv\n",
      "Split train and test datasets saved as split_train_dataset.csv and split_test_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/kaggle/input/kpdlhlv-raw-training/articles_training.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Save the raw dataset before processing\n",
    "# df.to_csv(\"raw_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Raw dataset saved as raw_dataset.csv\")\n",
    "\n",
    "# Function to tokenize and tag\n",
    "def convert_to_ner_format(content, tags):\n",
    "    tokens = re.split(r\"(\\s+|\\W+)\", content)\n",
    "    tokens = [t for t in tokens if t.strip()]  # Remove empty tokens\n",
    "    tags_set = set(tags.split(\",\"))\n",
    "    \n",
    "    # Initialize tags as 'O'\n",
    "    ner_tags = [\"O\"] * len(tokens)\n",
    "    \n",
    "    # Assign B-KEYWORD and I-KEYWORD\n",
    "    for entity in tags_set:\n",
    "        entity_tokens = entity.split()\n",
    "        for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "            if tokens[i:i + len(entity_tokens)] == entity_tokens:\n",
    "                ner_tags[i] = \"B-KEYWORD\"\n",
    "                if len(entity_tokens) > 1:\n",
    "                    for j in range(1, len(entity_tokens)):\n",
    "                        ner_tags[i + j] = \"I-KEYWORD\"\n",
    "    \n",
    "    return tokens, ner_tags\n",
    "\n",
    "# Convert data to NER format\n",
    "ner_dataset = []\n",
    "for idx, row in df.iterrows():\n",
    "    tokens, ner_tags = convert_to_ner_format(row[\"content\"], row[\"tags\"])\n",
    "    ner_dataset.append({\"id\": idx + 1, \"tokens\": tokens, \"ner_tags\": ner_tags, \"tags\": row[\"tags\"]})\n",
    "\n",
    "# Convert to DataFrame\n",
    "ner_df = pd.DataFrame({\n",
    "    \"id\": [sample[\"id\"] for sample in ner_dataset],\n",
    "    \"tokens\": [json.dumps(sample[\"tokens\"], ensure_ascii=False) for sample in ner_dataset],\n",
    "    \"ner_tags\": [json.dumps(sample[\"ner_tags\"], ensure_ascii=False) for sample in ner_dataset],\n",
    "    \"tags\": [sample[\"tags\"] for sample in ner_dataset]\n",
    "})\n",
    "\n",
    "# Save the raw train and test datasets\n",
    "split_ratio = 0.1\n",
    "test_df = ner_df.sample(frac=split_ratio, random_state=42)\n",
    "train_df = ner_df.drop(test_df.index)\n",
    "\n",
    "train_df.to_csv(\"raw_train.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(\"raw_test.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Raw train and test datasets saved as raw_train.csv and raw_test.csv\")\n",
    "\n",
    "# Sentence-based splitting function\n",
    "def split_text_by_sentence_with_ids(tokens, ner_tags, original_id, max_length=256):\n",
    "    split_data = []\n",
    "    segment_id = 1\n",
    "    i = 0\n",
    "    non_word_regex = re.compile(r'\\W')\n",
    "\n",
    "    while i < len(tokens):\n",
    "        end_idx = min(i + max_length, len(tokens))\n",
    "        current_tokens = tokens[i:end_idx]\n",
    "        current_tags = ner_tags[i:end_idx]\n",
    "\n",
    "        if len(current_tokens) == max_length:\n",
    "            split_idx = -1\n",
    "            for idx, token in enumerate(current_tokens):\n",
    "                if \".\" in token:\n",
    "                    split_idx = idx\n",
    "                elif non_word_regex.search(token) and split_idx == -1:\n",
    "                    split_idx = idx\n",
    "\n",
    "            if split_idx != -1:\n",
    "                end_idx = i + split_idx + 1\n",
    "                current_tokens = tokens[i:end_idx]\n",
    "                current_tags = ner_tags[i:end_idx]\n",
    "            else:\n",
    "                raise ValueError(f\"No valid boundary found to split within max_length at position {i}. Please check the input.\")\n",
    "\n",
    "        split_data.append({\n",
    "            \"id\": f\"{original_id}-{segment_id}\",\n",
    "            \"tokens\": current_tokens,\n",
    "            \"ner_tags\": current_tags,\n",
    "        })\n",
    "        segment_id += 1\n",
    "        i = end_idx\n",
    "\n",
    "    return split_data\n",
    "\n",
    "# Apply splitting\n",
    "def apply_splitting_to_dataset(df):\n",
    "    split_dataset = []\n",
    "    for idx, row in df.iterrows():\n",
    "        tokens = json.loads(row[\"tokens\"])\n",
    "        ner_tags = json.loads(row[\"ner_tags\"])\n",
    "        splits = split_text_by_sentence_with_ids(tokens, ner_tags, original_id=row[\"id\"])\n",
    "        split_dataset.extend(splits)\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [sample[\"id\"] for sample in split_dataset],\n",
    "        \"tokens\": [json.dumps(sample[\"tokens\"], ensure_ascii=False) for sample in split_dataset],\n",
    "        \"ner_tags\": [json.dumps(sample[\"ner_tags\"], ensure_ascii=False) for sample in split_dataset],\n",
    "    })\n",
    "\n",
    "# Save split datasets\n",
    "split_train_df = apply_splitting_to_dataset(train_df)\n",
    "split_test_df = apply_splitting_to_dataset(test_df)\n",
    "\n",
    "split_train_df.to_csv(\"split_train_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "split_test_df.to_csv(\"split_test_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Split train and test datasets saved as split_train_dataset.csv and split_test_dataset.csv\")\n",
    "\n",
    "# Integrate splits and include tags\n",
    "def integrate_splits(split_df, original_df):\n",
    "    split_df[\"original_id\"] = split_df[\"id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "    integrated_data = defaultdict(lambda: {\"tokens\": [], \"ner_tags\": [], \"tags\": \"\"})\n",
    "    for _, row in split_df.iterrows():\n",
    "        original_id = row[\"original_id\"]\n",
    "        tokens = json.loads(row[\"tokens\"])\n",
    "        ner_tags = json.loads(row[\"ner_tags\"])\n",
    "        integrated_data[original_id][\"tokens\"].extend(tokens)\n",
    "        integrated_data[original_id][\"ner_tags\"].extend(ner_tags)\n",
    "        integrated_data[original_id][\"tags\"] = original_df.loc[original_df[\"id\"] == int(original_id), \"tags\"].values[0]\n",
    "\n",
    "    return pd.DataFrame([\n",
    "        {\"id\": original_id,\n",
    "         \"tokens\": json.dumps(data[\"tokens\"], ensure_ascii=False),\n",
    "         \"ner_tags\": json.dumps(data[\"ner_tags\"], ensure_ascii=False),\n",
    "         \"tags\": data[\"tags\"]}\n",
    "        for original_id, data in integrated_data.items()\n",
    "    ])\n",
    "\n",
    "# integrated_train_df = integrate_splits(split_train_df, train_df)\n",
    "# integrated_test_df = integrate_splits(split_test_df, test_df)\n",
    "\n",
    "# integrated_train_df.to_csv(\"integrated_train_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "# integrated_test_df.to_csv(\"integrated_test_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "# print(\"Integrated train and test datasets saved as integrated_train_dataset.csv and integrated_test_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6189572,
     "sourceId": 10046737,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 437.300295,
   "end_time": "2024-12-03T14:18:55.287068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-03T14:11:37.986773",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
